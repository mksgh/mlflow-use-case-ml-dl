{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK,Trials,fmin,hp,tpe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## load the dataset\n",
    "data=pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training,validation and test sets\n",
    "\n",
    "train,test=train_test_split(data,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train.drop(['quality'],axis=1).values\n",
    "train_y=train[['quality']].values.ravel()\n",
    "\n",
    "## test dataset\n",
    "test_x=test.drop(['quality'],axis=1).values\n",
    "test_y=test[['quality']].values.ravel()\n",
    "\n",
    "## splitting this train data into train and validation\n",
    "\n",
    "train_x,valid_x,train_y,valid_y=train_test_split(train_x,train_y,test_size=0.20,random_state=42)\n",
    "\n",
    "signature=infer_signature(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANN Model\n",
    "\n",
    "def train_model(params,epochs,train_x,train_y,valid_x,valid_y,test_x,test_y):\n",
    "\n",
    "    ## Define model architecture\n",
    "    mean=np.mean(train_x,axis=0)\n",
    "    var=np.var(train_x,axis=0)\n",
    "\n",
    "    model=keras.Sequential(\n",
    "        [\n",
    "            keras.Input([train_x.shape[1]]),\n",
    "            keras.layers.Normalization(mean=mean,variance=var),\n",
    "            keras.layers.Dense(128,activation='relu'),\n",
    "            keras.layers.Dense(1)\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ## compile the model\n",
    "    model.compile(optimizer=keras.optimizers.SGD(\n",
    "                    learning_rate=params[\"lr\"],\n",
    "                    momentum=params[\"momentum\"]),\n",
    "                  loss=\"mean_squared_error\",\n",
    "                  metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "    ## Train the ANN model with lr and momentum params wwith MLFLOW tracking\n",
    "    with mlflow.start_run(nested=True):\n",
    "        model.fit(train_x,train_y,validation_data=(valid_x,valid_y),\n",
    "                  epochs=epochs,\n",
    "                  batch_size=64)\n",
    "        \n",
    "        ## Evaluate the model\n",
    "        eval_result=model.evaluate(valid_x,valid_y,batch_size=64)\n",
    "\n",
    "        eval_rmse=eval_result[1]\n",
    "\n",
    "        ## Log the parameters and results\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\",eval_rmse)\n",
    "\n",
    "        ## log the model\n",
    "\n",
    "        mlflow.tensorflow.log_model(model,\"model\",signature=signature)\n",
    "\n",
    "        return {\"loss\": eval_rmse, \"status\": STATUS_OK, \"model\": model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # MLflow will track the parameters and results for each run\n",
    "    result = train_model(\n",
    "        params,\n",
    "        epochs=3,\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        valid_x=valid_x,\n",
    "        valid_y=valid_y,\n",
    "        test_x=test_x,\n",
    "        test_y=test_y,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={\n",
    "        \"lr\" : hp.loguniform(\"lr\",np.log(1e-5),np.log(1e-1)),\n",
    "        \"momentum\" : hp.uniform(\"momentum\",0.0,1.0)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3                                            \n",
      "\n",
      " 1/46 [..............................] - ETA: 7s - loss: 37.6961 - root_mean_squared_error: 6.1397\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 4.5770 - root_mean_squared_error: 2.1394 - val_loss: 0.5153 - val_root_mean_squared_error: 0.7179\n",
      "\n",
      "Epoch 2/3                                            \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.5094 - root_mean_squared_error: 0.7138\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.5416 - root_mean_squared_error: 0.7359 - val_loss: 0.5069 - val_root_mean_squared_error: 0.7119\n",
      "\n",
      "Epoch 3/3                                            \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.4313 - root_mean_squared_error: 0.6567\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.5187 - root_mean_squared_error: 0.7202 - val_loss: 0.5265 - val_root_mean_squared_error: 0.7256\n",
      "\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.5131 - root_mean_squared_error: 0.7163\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.5265 - root_mean_squared_error: 0.7256\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?trial/s, best loss=?]INFO:tensorflow:Assets written to: C:\\Users\\mksgh\\AppData\\Local\\Temp\\tmpwfe3m4la\\model\\data\\model\\assets\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 14s - loss: 35.3993 - root_mean_squared_error: 5.9497\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 36.0732 - root_mean_squared_error: 6.0061 \n",
      "46/46 [==============================] - 1s 5ms/step - loss: 35.7817 - root_mean_squared_error: 5.9818 - val_loss: 35.2663 - val_root_mean_squared_error: 5.9385\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 37.6849 - root_mean_squared_error: 6.1388\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 34.9552 - root_mean_squared_error: 5.9123\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 34.7449 - root_mean_squared_error: 5.8945 - val_loss: 34.2480 - val_root_mean_squared_error: 5.8522\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 31.1622 - root_mean_squared_error: 5.5823\n",
      "42/46 [==========================>...] - ETA: 0s - loss: 33.7714 - root_mean_squared_error: 5.8113\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 33.7411 - root_mean_squared_error: 5.8087 - val_loss: 33.2624 - val_root_mean_squared_error: 5.7674\n",
      "\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 33.6793 - root_mean_squared_error: 5.8034\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 33.2624 - root_mean_squared_error: 5.7674\n",
      "\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:13<00:38, 12.75s/trial, best loss: 0.7255799770355225]INFO:tensorflow:Assets written to: C:\\Users\\mksgh\\AppData\\Local\\Temp\\tmpzlhbtszd\\model\\data\\model\\assets\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 14s - loss: 35.5745 - root_mean_squared_error: 5.9644\n",
      "39/46 [========================>.....] - ETA: 0s - loss: 7.9592 - root_mean_squared_error: 2.8212  \n",
      "46/46 [==============================] - 1s 5ms/step - loss: 7.6968 - root_mean_squared_error: 2.7743 - val_loss: 8.2334 - val_root_mean_squared_error: 2.8694\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 6.8593 - root_mean_squared_error: 2.6190\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 2.7778 - root_mean_squared_error: 1.6667\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 2.4541 - root_mean_squared_error: 1.5666 - val_loss: 0.9064 - val_root_mean_squared_error: 0.9520\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.7499 - root_mean_squared_error: 0.8659\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 0.7857 - root_mean_squared_error: 0.8864\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.7662 - root_mean_squared_error: 0.8754 - val_loss: 0.6164 - val_root_mean_squared_error: 0.7851\n",
      "\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.5498 - root_mean_squared_error: 0.7415\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6164 - root_mean_squared_error: 0.7851\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:22<00:20, 10.13s/trial, best loss: 0.7255799770355225]INFO:tensorflow:Assets written to: C:\\Users\\mksgh\\AppData\\Local\\Temp\\tmpwx1qhclc\\model\\data\\model\\assets\n",
      "Epoch 1/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 13s - loss: 35.6450 - root_mean_squared_error: 5.9703\n",
      "37/46 [=======================>......] - ETA: 0s - loss: 5.3200 - root_mean_squared_error: 2.3065  \n",
      "46/46 [==============================] - 1s 5ms/step - loss: 4.5355 - root_mean_squared_error: 2.1297 - val_loss: 1.2791 - val_root_mean_squared_error: 1.1310\n",
      "\n",
      "Epoch 2/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 1.1177 - root_mean_squared_error: 1.0572\n",
      "38/46 [=======================>......] - ETA: 0s - loss: 0.9886 - root_mean_squared_error: 0.9943\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.9859 - root_mean_squared_error: 0.9929 - val_loss: 0.8831 - val_root_mean_squared_error: 0.9397\n",
      "\n",
      "Epoch 3/3                                                                      \n",
      "\n",
      " 1/46 [..............................] - ETA: 0s - loss: 0.7740 - root_mean_squared_error: 0.8798\n",
      "41/46 [=========================>....] - ETA: 0s - loss: 0.7488 - root_mean_squared_error: 0.8653\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.7397 - root_mean_squared_error: 0.8601 - val_loss: 0.6766 - val_root_mean_squared_error: 0.8226\n",
      "\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.6200 - root_mean_squared_error: 0.7874\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.6766 - root_mean_squared_error: 0.8226\n",
      "\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:30<00:09,  9.33s/trial, best loss: 0.7255799770355225]INFO:tensorflow:Assets written to: C:\\Users\\mksgh\\AppData\\Local\\Temp\\tmp1hscc5x_\\model\\data\\model\\assets\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:37<00:00,  9.39s/trial, best loss: 0.7255799770355225]\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\mksgh\\AppData\\Local\\Temp\\tmp6wuipgub\\model\\data\\model\\assets\n",
      "Best parameters: {'lr': 0.043145521758735816, 'momentum': 0.8268098606156861}\n",
      "Best eval rmse: 0.7255799770355225\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"wine-quality\")\n",
    "with mlflow.start_run():\n",
    "    # Conduct the hyperparameter search using Hyperopt\n",
    "    trials=Trials()\n",
    "    best=fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=4,\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Fetch the details of the best run\n",
    "    best_run = sorted(trials.results, key=lambda x: x[\"loss\"])[0]\n",
    "\n",
    "    # Log the best parameters, loss, and model\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\", best_run[\"loss\"])\n",
    "    mlflow.tensorflow.log_model(best_run[\"model\"], \"model\", signature=signature)\n",
    "\n",
    "    # Print out the best parameters and corresponding loss\n",
    "    print(f\"Best parameters: {best}\")\n",
    "    print(f\"Best eval rmse: {best_run['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/01 15:24:20 INFO mlflow.models.python_api: It is highly recommended to use `uv` as the environment manager for predicting with MLflow models as its performance is significantly better than other environment managers. Run `pip install uv` to install uv. See https://docs.astral.sh/uv/getting-started/installation for other installation methods.\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 2197.23it/s]\n",
      "2025/02/01 15:24:20 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 787us/step\n",
      "{\"predictions\": [[6.098855018615723], [7.176075458526611], [6.630058765411377], [5.681721210479736], [6.632232666015625], [5.810438632965088], [5.366516590118408], [5.206330299377441], [5.7631988525390625], [5.53447961807251], [6.412518501281738], [5.090635299682617], [6.8876848220825195], [5.536290645599365], [6.564769268035889], [5.442131996154785], [6.615154266357422], [6.01826810836792], [6.269148826599121], [5.589300632476807], [5.541668891906738], [6.001195907592773], [5.568359851837158], [6.318166732788086], [6.046114444732666], [5.579759120941162], [5.349600315093994], [6.376130104064941], [6.15510368347168], [5.483193874359131], [5.700394630432129], [5.790901184082031], [5.625024318695068], [5.515038967132568], [5.77320671081543], [6.768563270568848], [6.358737945556641], [5.375560283660889], [5.552730083465576], [5.770209789276123], [5.711782932281494], [5.666099548339844], [6.105942726135254], [5.888269424438477], [5.382665634155273], [5.913486480712891], [6.517630100250244], [5.422181606292725], [5.768097877502441], [5.667671203613281], [4.955062389373779], [5.756750106811523], [5.5163702964782715], [6.351768970489502], [6.0841498374938965], [5.903657913208008], [5.837064266204834], [5.937230587005615], [5.805072784423828], [6.193539619445801], [6.093414783477783], [6.1405463218688965], [5.387746810913086], [6.9774932861328125], [6.270237922668457], [6.428306579589844], [6.200328350067139], [6.018362998962402], [5.973559379577637], [6.0626349449157715], [5.256516933441162], [6.066197395324707], [5.212696075439453], [4.785796165466309], [5.854634761810303], [6.436691761016846], [5.845513343811035], [5.840306758880615], [6.771020889282227], [5.9679975509643555], [6.103050231933594], [5.551164150238037], [6.160212993621826], [5.865108013153076], [6.571155548095703], [6.506546974182129], [5.553386688232422], [5.693183898925781], [6.096827983856201], [5.792656421661377], [6.664796829223633], [5.8075852394104], [5.849085807800293], [6.585379600524902], [6.53065824508667], [6.534196853637695], [6.5426836013793945], [5.701507568359375], [5.7035698890686035], [6.968307971954346], [5.69296932220459], [6.153269290924072], [6.439244270324707], [6.096249580383301], [5.933073997497559], [6.066989898681641], [6.193539619445801], [6.309684753417969], [6.01826810836792], [6.30268669128418], [5.857893943786621], [5.502481937408447], [6.796220302581787], [6.115528583526611], [5.418415069580078], [7.0777812004089355], [6.842951774597168], [6.100220203399658], [5.383542537689209], [6.585755825042725], [5.537259578704834], [6.743924617767334], [6.258726596832275], [5.44119119644165], [6.0402302742004395], [5.938616752624512], [6.991281509399414], [5.788155555725098], [6.566307067871094], [5.530899524688721], [6.023590564727783], [5.7226152420043945], [5.545199394226074], [6.145864009857178], [5.328219890594482], [5.995434284210205], [6.029143810272217], [5.493791103363037], [6.637903213500977], [5.579586982727051], [6.856880187988281], [5.700544357299805], [6.28632926940918], [5.7192487716674805], [6.217541217803955], [5.526656150817871], [5.443826198577881], [5.715745449066162], [5.5093770027160645], [5.706040382385254], [6.361509323120117], [5.545897960662842], [6.192129611968994], [6.145716190338135], [6.376641750335693], [5.038748264312744], [6.718045234680176], [5.949163436889648], [6.370865821838379], [7.319289207458496], [5.31752872467041], [5.41979455947876], [5.812142372131348], [6.484188556671143], [6.110300064086914], [6.716325759887695], [4.988689422607422], [5.788387298583984], [5.408616065979004], [6.032052516937256], [5.254610538482666], [6.1583709716796875], [6.424447536468506], [5.870450019836426], [7.202289581298828], [5.85590934753418], [5.407311916351318], [6.220727920532227], [6.077103137969971], [5.479236602783203], [6.2246246337890625], [5.8202643394470215], [5.914582252502441], [6.200201988220215], [5.631145000457764], [5.782113552093506], [6.597768783569336], [5.787379264831543], [5.963777542114258], [4.978848934173584], [5.402450084686279], [5.802456378936768], [6.06759786605835], [6.659512519836426], [5.211531639099121], [5.28423547744751], [7.1430535316467285], [6.8535003662109375], [5.582667350769043], [6.0987653732299805], [6.049314498901367], [4.798749923706055], [6.718766689300537], [5.999157428741455], [5.4578657150268555], [5.957936763763428], [6.981822490692139], [6.268220901489258], [5.4540486335754395], [6.4658122062683105], [5.910391807556152], [6.427081108093262], [6.209939479827881], [6.829306602478027], [4.97996711730957], [5.630353927612305], [5.730830192565918], [5.111790657043457], [6.1405463218688965], [6.589383125305176], [5.657220363616943], [5.512299537658691], [5.871150493621826], [6.487016677856445], [6.34520149230957], [5.386678695678711], [6.021913051605225], [6.931745529174805], [6.228635311126709], [5.359524726867676], [5.730388641357422], [5.640504360198975], [6.32177734375], [5.509319305419922], [5.390164852142334], [7.2913947105407715], [6.122500419616699], [6.146172046661377], [5.935131549835205], [5.614343166351318], [5.772764682769775], [5.649445533752441], [6.9109954833984375], [6.066347599029541], [6.0356764793396], [5.184420585632324], [5.114223480224609], [5.6631879806518555], [5.953248500823975], [5.346479892730713], [5.753493785858154], [5.629079341888428], [5.774142742156982], [5.26777458190918], [5.826216220855713], [5.592700004577637], [6.046336650848389], [5.6243791580200195], [5.7419586181640625], [6.018409729003906], [6.011888027191162], [5.657069206237793], [5.116400241851807], [4.980129718780518], [6.637528896331787], [6.054336071014404], [5.908417701721191], [5.8600006103515625], [5.993494987487793], [5.75956392288208], [5.203912258148193], [6.203061103820801], [6.292755126953125], [6.943477630615234], [5.1300177574157715], [6.119246006011963], [5.612451553344727], [5.754036903381348], [6.3702239990234375], [6.20601224899292], [5.688582420349121], [5.854634761810303], [6.388070106506348], [5.0859832763671875], [6.178159713745117], [6.995126247406006], [7.496591567993164], [5.6028523445129395], [5.709447383880615], [5.540911674499512], [5.985190391540527], [5.731359481811523], [6.479373931884766], [5.65067195892334], [6.435703277587891], [6.081068515777588], [5.350543975830078], [5.930370330810547], [5.363677501678467], [5.693697452545166], [4.955062389373779], [5.379432678222656], [4.762540817260742], [5.35485315322876], [6.904033660888672], [5.518880367279053], [6.409265041351318], [5.855247974395752], [6.365688323974609], [5.837540626525879], [6.096022129058838], [6.152283668518066], [5.517516136169434], [6.1716227531433105], [6.1721086502075195], [7.196508407592773], [5.811124801635742], [6.611873626708984], [4.921865463256836], [6.862899303436279], [6.382306098937988], [6.409599781036377], [6.660314559936523], [6.372514724731445], [5.610321998596191], [5.284493446350098], [6.7903289794921875], [5.4587297439575195], [7.0444560050964355], [5.681077480316162], [6.13438606262207], [6.559062957763672], [6.4782023429870605], [5.736395835876465], [5.058099269866943], [5.968793869018555], [6.8191118240356445], [6.686321258544922], [6.186524391174316], [5.891673564910889], [5.563710689544678], [5.986964225769043], [5.477229595184326], [6.166005611419678], [5.236376762390137], [5.578248500823975], [6.468581199645996], [5.152698993682861], [6.603723049163818], [6.50068473815918], [6.906786918640137], [6.369431495666504], [5.933073997497559], [5.513935089111328], [6.5319037437438965], [5.26777458190918], [5.6645827293396], [6.509370803833008], [6.959589004516602], [5.550116539001465], [5.278084754943848], [5.460206031799316], [5.671548366546631], [5.640719413757324], [6.024261951446533], [5.246971607208252], [5.837780952453613], [5.631208896636963], [5.940830230712891], [6.202273845672607], [5.385528564453125], [5.515869140625], [5.868649005889893], [5.946868419647217], [6.416828155517578], [5.898233890533447], [6.068955421447754], [5.140885353088379], [6.290409088134766], [5.990117073059082], [6.114502906799316], [6.572751522064209], [5.9154486656188965], [6.084308624267578], [5.893256664276123], [5.77476167678833], [6.706809997558594], [5.876176834106445], [6.108921527862549], [5.131516456604004], [5.329686164855957], [6.0833940505981445], [5.423671722412109], [5.454101085662842], [4.912741661071777], [5.100015640258789], [4.91471004486084], [6.274068832397461], [6.283517837524414], [5.731150150299072], [5.761748790740967], [6.065669536590576], [6.019001483917236], [6.419023513793945], [5.778444766998291], [6.486332416534424], [6.248938083648682], [5.294106960296631], [6.71920919418335], [6.9805402755737305], [5.65067195892334], [5.711026191711426], [7.178336143493652], [5.870259761810303], [5.671519756317139], [6.258662700653076], [5.126976013183594], [5.934475898742676], [5.1873273849487305], [6.047182083129883], [6.513942241668701], [5.762857913970947], [5.638601303100586], [6.579962253570557], [6.9945597648620605], [6.063559055328369], [5.637073993682861], [6.493764877319336], [6.166896343231201], [6.132360458374023], [6.531528949737549], [5.888726711273193], [5.923861026763916], [6.571267604827881], [6.634518623352051], [5.463519096374512], [6.281752586364746], [6.431824684143066], [5.485651969909668], [6.087033271789551], [5.61367654800415], [6.8959455490112305], [6.436691761016846], [5.337327003479004], [5.474960803985596], [6.031945705413818], [7.055811405181885], [6.1726250648498535], [5.447186470031738], [5.808351039886475], [5.548131465911865], [5.707211494445801], [5.578014373779297], [6.655704975128174], [6.55624532699585], [5.788436412811279], [5.4188642501831055], [6.665661334991455], [4.884323596954346], [5.0720319747924805], [5.463831424713135], [6.585312843322754], [5.768130779266357], [5.84411096572876], [6.195136070251465], [6.746524333953857], [5.668664455413818], [5.745723247528076], [6.807473182678223], [4.577114582061768], [5.261153221130371], [6.037474632263184], [6.398192405700684], [5.49530029296875], [4.815609931945801], [5.626006603240967], [5.507057189941406], [5.700019836425781], [5.972687721252441], [6.432544708251953], [6.3247222900390625], [5.038720607757568], [5.459338665008545], [5.528582572937012], [4.897012233734131], [6.836491584777832], [5.9247002601623535], [5.482855319976807], [5.532219886779785], [5.637288570404053], [5.699273109436035], [5.447706699371338], [6.617724895477295], [5.909270286560059], [6.318516731262207], [6.806480884552002], [5.969895839691162], [5.592700004577637], [5.618309020996094], [5.11275577545166], [6.5141682624816895], [5.835292339324951], [5.656347751617432], [6.455570220947266], [5.828463554382324], [5.994131565093994], [5.902596950531006], [5.966810703277588], [6.138856410980225], [6.813456058502197], [6.378705978393555], [5.022284984588623], [5.500241279602051], [5.672275543212891], [5.9670634269714355], [5.5682244300842285], [5.811311721801758], [5.481934070587158], [6.100456714630127], [4.847247123718262], [5.389517784118652], [5.040184497833252], [5.671414852142334], [6.1338396072387695], [5.953753471374512], [5.790901184082031], [5.892801284790039], [6.027297496795654], [5.8376145362854], [3.5233545303344727], [6.918544769287109], [5.9950127601623535], [5.553386688232422], [5.412607669830322], [5.983118057250977], [5.564435005187988], [6.06223201751709], [6.473470211029053], [6.99256706237793], [5.873786449432373], [5.518283843994141], [5.482108116149902], [6.367228031158447], [5.563185214996338], [6.392030239105225], [5.583981990814209], [6.10357141494751], [6.376699447631836], [6.016883373260498], [6.22001838684082], [5.88295316696167], [6.87153434753418], [6.766136169433594], [5.8828558921813965], [7.202289581298828], [6.727860450744629], [7.073751449584961], [7.074317932128906], [5.6282501220703125], [5.662231922149658], [6.789020538330078], [6.247053623199463], [5.899914741516113], [5.463149070739746], [6.419310092926025], [5.297771453857422], [5.577426433563232], [5.94814920425415], [6.182321071624756], [5.685426712036133], [5.9266037940979], [5.901888847351074], [6.278901100158691], [6.114867687225342], [6.216305732727051], [5.748702049255371], [5.835583686828613], [6.138856410980225], [6.43511962890625], [6.399336814880371], [5.3140153884887695], [6.449787139892578], [6.830322742462158], [5.360428333282471], [5.85430908203125], [5.616838455200195], [6.552619934082031], [5.594722747802734], [5.978419780731201], [5.994917869567871], [6.290409088134766], [6.574057579040527], [5.554659366607666], [6.693026065826416], [5.660552024841309], [6.2856526374816895], [6.773573398590088], [6.603522300720215], [5.635367393493652], [5.520768165588379], [5.768097877502441], [6.436068534851074], [6.062863349914551], [5.143047332763672], [5.911258220672607], [7.5450568199157715], [5.618209362030029], [5.7623291015625], [5.946868419647217], [6.601898670196533], [6.2849273681640625], [6.815828323364258], [6.132098197937012], [6.051457405090332], [6.500278472900391], [5.647350788116455], [5.898552417755127], [6.359488487243652], [5.639729022979736], [5.324607849121094], [6.15510368347168], [6.929240703582764], [5.443611145019531], [6.115252494812012], [6.201766490936279], [5.5673065185546875], [6.409599781036377], [5.003932952880859], [5.333473205566406], [6.965846538543701], [5.385822772979736], [5.666099548339844], [6.535735607147217], [5.836946964263916], [5.446693420410156], [5.4213361740112305], [6.067840576171875], [5.308681488037109], [6.319307327270508], [6.141263484954834], [5.577175140380859], [6.597049713134766], [6.460820198059082], [4.932065010070801], [5.702878952026367], [5.404153347015381], [6.186067581176758], [5.652127742767334], [6.676856517791748], [5.957425594329834], [6.959277153015137], [6.785171031951904], [6.053144454956055], [6.041250228881836], [6.727860450744629], [6.22001838684082], [5.673278331756592], [5.341061592102051], [5.832273483276367], [5.455165863037109], [6.587708950042725], [6.511476993560791], [6.495758056640625], [6.318166732788086], [5.666455268859863], [6.302785396575928], [5.890113830566406], [5.414578914642334], [6.404323577880859], [5.751449108123779], [6.125415325164795], [6.414639472961426], [5.577841281890869], [5.6145339012146], [6.636590003967285], [5.9456892013549805], [5.363283634185791], [6.693026065826416], [5.406811237335205], [6.222311973571777], [6.185051441192627], [5.79992151260376], [6.049131393432617], [6.2437028884887695], [6.106573581695557], [6.141399383544922], [6.248373031616211], [5.5185418128967285], [6.098306179046631], [6.29905891418457], [5.558815002441406], [5.5419721603393555], [5.667671203613281], [7.01014518737793], [6.690886497497559], [6.587708950042725], [5.8958821296691895], [5.844597816467285], [6.435475826263428], [6.409580230712891], [5.298579692840576], [5.800939083099365], [5.805503845214844], [6.790408134460449], [6.090656757354736], [5.835192680358887], [7.286698818206787], [7.031009674072266], [5.7032694816589355], [5.994933128356934], [5.203249454498291], [6.5916924476623535], [6.729888439178467], [6.79807186126709], [5.470029354095459], [5.409918308258057], [5.726797103881836], [6.3084492683410645], [5.470226287841797], [6.336579322814941], [5.3762288093566895], [5.016657829284668], [5.25622034072876], [6.36313009262085], [6.0252509117126465], [5.808975696563721], [5.9708991050720215], [6.269189834594727], [6.346648693084717], [5.462388515472412], [5.2773284912109375], [5.957936763763428], [6.341513633728027], [5.575787544250488], [5.399312973022461], [5.535457134246826], [6.562439441680908], [6.071255207061768], [5.26870059967041], [6.6743083000183105], [6.343303203582764], [5.201344966888428], [6.673249244689941], [6.595599174499512], [6.876194477081299], [6.230411529541016], [5.893484592437744], [5.325761795043945], [6.621589660644531], [6.97849702835083], [6.236374855041504], [5.696793556213379], [6.772784233093262], [7.535740852355957], [5.4795074462890625], [5.251466274261475], [6.145716190338135], [5.86365270614624], [5.2415642738342285], [6.076330661773682], [5.717222213745117], [5.827861785888672], [5.957531929016113], [6.304980278015137], [6.341527462005615], [6.674732208251953], [5.582662105560303], [6.42700719833374], [5.966398239135742], [5.568673133850098], [5.4313883781433105], [6.517495632171631], [5.388046741485596], [6.254481792449951], [5.535232067108154], [6.789020538330078], [6.6178975105285645], [5.984745502471924], [7.323757648468018], [5.865121841430664], [6.052478790283203], [6.140752792358398], [6.071864128112793], [5.622284889221191], [6.422213554382324], [6.339820861816406], [6.856266975402832], [5.391613006591797], [5.260514259338379], [6.523148059844971], [5.674408435821533], [6.556726455688477], [5.640002727508545], [5.76618766784668], [5.593012809753418], [6.901794910430908], [7.044475555419922], [6.157594680786133], [5.081901550292969], [5.671027183532715], [6.456493854522705], [6.077045917510986], [5.668593406677246], [5.3121466636657715], [6.200023174285889], [5.62337064743042], [6.123120307922363], [5.373483180999756], [6.7110161781311035], [6.00364875793457], [5.926383972167969], [6.203237056732178], [5.933570861816406], [6.721041202545166], [5.652851581573486], [5.929005146026611], [6.188337326049805], [5.127222061157227], [5.371179580688477], [7.0315775871276855], [6.814722061157227], [5.317051887512207], [6.451666831970215], [6.158309459686279], [5.168745040893555], [5.933570861816406], [7.335438251495361], [5.316096782684326], [5.363847732543945], [6.026323318481445], [6.217541217803955], [5.148225784301758], [6.171342372894287], [6.248938083648682], [6.393001556396484], [5.87217903137207], [5.882780075073242], [5.472243309020996], [5.9835333824157715], [6.083050727844238], [6.678440570831299], [6.095047473907471], [5.631235599517822], [6.070003986358643], [6.187596321105957], [5.754781723022461], [6.059667110443115], [6.01092529296875], [5.864875793457031], [5.899508953094482], [5.96734094619751], [6.151906490325928], [6.5704498291015625], [6.717776298522949], [5.959169864654541], [5.903816223144531], [5.825075626373291], [6.128495693206787], [5.606374740600586], [6.222907066345215], [5.326032638549805], [5.388046741485596], [5.279118537902832], [6.517489910125732], [6.218935489654541], [6.901274681091309], [6.499924659729004], [6.012389659881592], [5.830684661865234], [5.989593029022217], [5.481847286224365], [6.315639495849609], [7.20613956451416], [5.373483180999756], [5.455526828765869], [5.830516815185547], [6.678649425506592], [6.520334243774414], [5.861146926879883], [6.360573768615723], [5.559412956237793], [5.965816020965576], [6.747615814208984], [6.44327974319458], [6.22218132019043], [6.52752161026001], [6.051358222961426], [5.7227253913879395], [5.605306148529053], [6.072997093200684], [5.4822282791137695], [6.432082176208496], [6.412322044372559], [5.876158237457275], [5.5682244300842285], [6.907058238983154], [6.383991241455078], [5.939582824707031], [5.5143890380859375], [5.912240028381348], [5.3070597648620605], [5.984745502471924], [5.731903553009033], [5.645978927612305], [4.728811740875244], [6.0987653732299805], [6.358263969421387], [6.561973571777344], [6.207158088684082], [5.898233890533447], [5.915252208709717], [5.500580787658691], [6.059077739715576], [5.936637878417969], [6.5950751304626465], [6.359488487243652], [5.201576232910156], [6.274715423583984], [5.644211769104004], [6.817715644836426], [5.550488471984863], [5.8826446533203125], [6.372523784637451], [7.47391414642334], [6.690032482147217], [6.17603874206543], [7.285731315612793], [6.031219959259033], [5.582667350769043], [5.523097515106201], [6.636950492858887], [5.8500189781188965], [6.418189525604248], [6.025428771972656], [5.324470520019531], [6.541780948638916], [5.488272190093994], [5.535232067108154], [5.830684661865234], [6.045365810394287], [5.37401008605957], [7.071707725524902], [6.3670783042907715], [5.684632778167725], [5.693241596221924], [5.51083517074585], [5.420405864715576], [5.582836151123047], [6.01215124130249], [5.57034158706665], [5.572460174560547], [5.376381874084473], [5.800811290740967], [6.489936828613281], [6.155099391937256], [6.878911972045898], [6.914314270019531], [6.210104465484619], [5.396757125854492], [6.711575984954834], [6.893343925476074], [5.536996364593506], [6.793378829956055], [4.931014060974121], [5.482368469238281], [5.62959623336792], [6.052966594696045], [5.9154486656188965], [6.677539825439453], [4.859212398529053], [6.295648574829102], [5.827479839324951], [5.332594394683838], [5.474635124206543], [6.119459629058838], [5.84788179397583], [6.24378776550293], [5.452063083648682], [5.5339789390563965], [5.719694137573242], [6.815351486206055], [5.482778549194336], [6.366968154907227], [6.1831583976745605], [5.823250770568848], [5.656564712524414], [6.134531497955322], [5.758635997772217], [6.359488487243652], [5.7998456954956055], [6.5124125480651855], [6.149784088134766], [5.151551723480225], [5.655178070068359], [5.684854030609131], [5.946868419647217], [6.711977005004883], [5.933073997497559], [5.698605060577393], [6.027801990509033], [5.470994472503662], [5.93876838684082], [6.476024150848389], [5.578280448913574], [6.24378776550293], [6.281047821044922], [5.489448070526123], [5.936875343322754], [6.402621269226074], [6.35911226272583], [6.1343889236450195], [5.100435256958008], [6.08301305770874], [5.428833961486816], [6.206768035888672], [6.564615249633789], [5.5040740966796875], [5.571084976196289], [6.343303203582764], [5.085521221160889], [6.193539619445801], [6.361231803894043], [6.770551681518555], [5.477543354034424], [5.5163893699646], [6.76420783996582], [6.818223476409912], [5.835659503936768], [5.570647239685059], [5.424184799194336], [5.339719772338867], [6.1343889236450195], [6.035676956176758], [6.377885818481445], [5.957936763763428], [5.834530830383301], [6.097775936126709], [5.917209148406982], [5.392848968505859], [5.48573637008667], [6.378653526306152], [7.234323501586914], [5.459338665008545], [5.403320789337158], [5.658822059631348], [5.14664888381958], [5.502481937408447], [5.623624324798584], [7.052633285522461], [5.64823055267334], [5.9257283210754395], [5.047842025756836], [5.468809604644775], [5.832865238189697], [5.536450386047363], [5.484983921051025], [5.248852252960205], [5.419588088989258], [5.8939361572265625], [6.023507595062256], [5.707180023193359], [5.9603471755981445], [5.527121543884277], [5.323360443115234], [5.728489875793457], [6.068909645080566], [5.3627519607543945], [6.037776947021484], [6.636950492858887], [5.428292751312256], [6.459682941436768], [6.46380090713501], [5.9188947677612305], [5.958616733551025], [6.150986671447754], [6.711584091186523], [6.3397955894470215], [7.052534103393555], [6.071178913116455], [6.446155548095703], [5.48166036605835], [6.357888698577881], [5.481634140014648], [6.148333549499512], [6.995126247406006], [6.1066813468933105], [6.789020538330078], [6.669376850128174], [6.223785400390625], [5.531434535980225], [5.52132511138916], [6.134609699249268], [5.5689921379089355], [5.9662184715271], [5.370201110839844], [5.716326713562012], [6.141773700714111], [4.965451240539551], [5.951197624206543], [5.5419721603393555], [5.9981842041015625], [5.949801921844482], [5.763223171234131], [6.6939616203308105], [5.938697814941406], [6.359488487243652], [5.7973480224609375], [5.880893707275391], [6.337010383605957], [5.450506210327148], [5.849597454071045], [5.608410358428955], [6.547509670257568], [5.2362823486328125], [5.377563953399658], [5.308440208435059], [6.047379016876221], [5.894967555999756], [6.016017913818359], [6.711575984954834], [6.383234024047852], [6.8110198974609375], [5.729864120483398], [5.46121072769165], [6.088695049285889], [6.409599781036377], [5.4978461265563965], [5.769399642944336], [6.405137062072754], [6.5750041007995605], [5.496957778930664], [6.1583709716796875], [6.642097473144531], [6.735297203063965], [6.7653279304504395], [5.493791103363037], [6.1228861808776855], [5.9025044441223145], [6.3135762214660645], [4.887382507324219], [5.88828182220459], [6.246952533721924], [6.59807825088501], [5.382178783416748], [6.5082316398620605], [6.908963680267334], [4.773351669311523], [5.363287925720215], [6.418090343475342], [6.300829887390137], [7.279922962188721], [5.533212661743164], [6.84946346282959], [5.434319496154785], [5.843125820159912], [5.781681060791016], [6.290409088134766], [6.1247148513793945], [5.664533615112305], [5.523054599761963], [5.69903564453125], [5.255890846252441], [6.336856842041016], [5.655391693115234], [6.417298316955566], [6.108234405517578], [6.489231586456299], [7.124099254608154], [6.9805402755737305], [6.986431121826172], [6.476502418518066], [5.905406951904297], [5.547296047210693], [5.820750713348389], [6.698843955993652], [5.431918144226074], [6.040191650390625], [6.985933303833008], [4.973294258117676], [6.290409088134766], [5.7998456954956055], [6.021693229675293], [5.853413105010986], [6.185426712036133], [7.09283447265625], [6.365293979644775], [6.2902374267578125], [4.842206001281738], [5.626499652862549], [6.788558006286621], [4.489175796508789], [6.04306697845459], [5.742218017578125], [6.259636402130127], [5.96734094619751], [5.721121311187744], [6.639657020568848], [6.322638511657715], [6.202901363372803], [6.270509719848633], [6.374044418334961], [5.685242652893066], [5.45366096496582], [5.529294967651367], [6.274972915649414], [6.108388423919678], [6.0859832763671875], [5.9570512771606445], [5.893630504608154], [5.944741249084473], [7.097405910491943], [6.082638263702393], [5.484001636505127], [5.577426433563232], [5.671300888061523], [5.34367036819458], [6.611179351806641], [6.607640266418457], [5.254942893981934], [5.789382457733154], [6.7176408767700195], [6.584339618682861], [6.453258514404297], [6.806460380554199], [5.964064121246338]]}"
     ]
    }
   ],
   "source": [
    "## Inferencing\n",
    "\n",
    "model_uri = 'runs:/171e623bc32c47e898cb22081a66f849/model'\n",
    "\n",
    "# Replace INPUT_EXAMPLE with your own input example to the model\n",
    "# A valid input example is a data instance suitable for pyfunc prediction\n",
    "input_data = test_x\n",
    "\n",
    "# Verify the model with the provided input data using the logged dependencies.\n",
    "# For more details, refer to:\n",
    "# https://mlflow.org/docs/latest/models.html#validate-models-before-deployment\n",
    "mlflow.models.predict( model_uri=model_uri, input_data=input_data, env_manager=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 974us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.098855 ],\n",
       "       [7.1760755],\n",
       "       [6.630059 ],\n",
       "       ...,\n",
       "       [6.4532585],\n",
       "       [6.8064604],\n",
       "       [5.964064 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model as a PyFuncModel.\n",
    "model_uri = 'runs:/171e623bc32c47e898cb22081a66f849/model'\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "loaded_model.predict(pd.DataFrame(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'wine-quality-final'.\n",
      "Created version '1' of model 'wine-quality-final'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1738403728621, current_stage='None', description=None, last_updated_timestamp=1738403728621, name='wine-quality-final', run_id='171e623bc32c47e898cb22081a66f849', run_link=None, source='file:///c:/Users/mksgh/OneDrive%20-%20Vestas%20Wind%20Systems%20A%20S/Documents/Github/mlflow-use-case-ml-dl/DL/mlruns/678676502690910251/171e623bc32c47e898cb22081a66f849/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Register in the model registry\n",
    "mlflow.register_model(model_uri,\"wine-quality-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mksgh\\OneDrive - Vestas Wind Systems A S\\Documents\\Github\\mlflow-use-case-ml-dl\\venv-mlflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 1571.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 763us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.098855 ],\n",
       "       [7.1760755],\n",
       "       [6.630059 ],\n",
       "       ...,\n",
       "       [6.4532585],\n",
       "       [6.8064604],\n",
       "       [5.964064 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Inferencing\n",
    "\n",
    "# from mlflow.models import validate_serving_input\n",
    "\n",
    "# model_uri = 'runs:/171e623bc32c47e898cb22081a66f849/model'\n",
    "\n",
    "# # The logged model does not contain an input_example.\n",
    "# # Manually generate a serving payload to verify your model prior to deployment.\n",
    "# from mlflow.models import convert_input_example_to_serving_input\n",
    "\n",
    "# # Define INPUT_EXAMPLE via assignment with your own input example to the model\n",
    "# # A valid input example is a data instance suitable for pyfunc prediction\n",
    "# serving_payload = convert_input_example_to_serving_input(test_x)\n",
    "\n",
    "# # Validate the serving payload works on the model\n",
    "# validate_serving_input(model_uri, serving_payload)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
